{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Crimpson'>Pre-requisites - From the `Feature Extraction` and `Images_as_numerical_data` Notebooks</font>\n",
    "- We understood what images are? Also looked into medical/dicom images.\n",
    "- We understood different pre-processing steps that are needed to clean image data\n",
    "- We learnt about the computer vision tasks - image classification, object detection, image segmentation etc\n",
    "- We learnt about how traditional machine learning works with image data and how to create hand generated features like - edge detection, computing mean/mode from histograms etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN's\n",
    "\n",
    "Let's assume we have an image with height =6, width =6, and the number of channels =3 (Coloured image). So there are 6x6x3 =108 numbers. Consider we have the first hidden layer of neural network with 10 units. The total number of parameters(Weights) are 108 x 10 =1080. So we need 1080 weights for only one layer and Generally, the size of the image will be equal to 224 x 224. In these kinds of cases, we get a lot of parameters to train which makes it computationally expensive and the model doesn't perform better.\n",
    "\n",
    "To deal with such problems, we have special neural networks called Convolutional Neural Networks (CNNs).\n",
    "\n",
    "A convolutional neural network is a type of neural network that is used in image processing and image classification. This neural network takes the pixels of an image as input and generates the desired output.\n",
    "\n",
    "Let’s understand how a CNN works:\n",
    "\n",
    "The first step of a CNN is to detect features like edges, shapes e.c.t.\n",
    "\n",
    "This is done by applying a convolution to the image using filters (Filters are responsible for detecting some kind of shape).\n",
    "\n",
    "Let’s understand this using an example:\n",
    "\n",
    "let’s take a 6 X 6 image\n",
    "\n",
    "\n",
    "![alt](IMG1.png)\n",
    "\n",
    "\n",
    "Next, we convolve this 6 X 6 matrix with a 3 X 3 filter:\n",
    "\n",
    "![alt](IMG2.png)\n",
    "\n",
    "After the convolution, we will get a 4 X 4 image. The first element of the 4 X 4 matrix will be calculated as:\n",
    "\n",
    "![alt](IMG3.png)\n",
    "\n",
    "So, we take the first 3 X 3 matrix from the 6 X 6 image and multiply it with the filter. The first element of the 4 X 4 matrix, will be the sum of the element-wise product of these values, i.e. 3*1 + 0 + 1*-1 + 1*1 + 5*0 + 8*-1 + 2*1 + 7*0 + 2*-1 = -5.\n",
    "\n",
    "Similarly, we will convolve over the entire image and get a 4 X 4 matrix.\n",
    "\n",
    "In convolution the total parameters = 3 x 3 x 3 (numbers in filters ) x 10(filters) +10(bias) =280. The number of weights very less compared to ANN .\n",
    "\n",
    "Filters\n",
    "\n",
    "Filters are responsible for locating objects in an image by detecting the changes in intensity values of the images.\n",
    "\n",
    "Generally, we have an edge detector that detects edges in an image. For example \n",
    "\n",
    "![alt](IMG4.png)\n",
    "\n",
    "This filter is responsible for detecting vertical edges and similarly, for detecting horizontal edges we use\n",
    "\n",
    "![alt](IMG5.png)\n",
    "\n",
    "So in images, we have a lot of complex features to get detected other than edges. For that purpose, we randomly choose filter values which model can learn itself during backpropagation.\n",
    "\n",
    "### Padding\n",
    "\n",
    "We have seen that convolving an input of 6 X 6 dimensions with a 3 X 3 filter results in a 4 X 4 matrix. We can generalize this and say that if the input is n X n and the filter size is f X f, then the output size will be (n-f+1) X (n-f+1):\n",
    "\n",
    "**Input size:** n X n <br>\n",
    "**Filter size:** f X f <br>\n",
    "**Output size:** (n-f+1) X (n-f+1)<br>\n",
    "\n",
    "But there are certain disadvantages of a convolutional filter: \n",
    "\n",
    "1. When we apply a convolutional filter, the size of the image reduces.\n",
    "2. Pixels that are present in the corner of the image are used only a few times during convolution when compared to the other pixels. This leads to data loss.\n",
    "\n",
    "To avoid these issues, we can add a border around the process. This border is also called padding. if we apply a padding of 1, it means that the input will be an 8 X 8 matrix (instead of a 6 X 6 matrix). Applying a convolution of 3 X 3 on it will result in a 6 X 6 matrix, which is the original shape of the image. \n",
    "\n",
    "**Input size:** n X n <br>\n",
    "**Padding size:** p <br>\n",
    "**Filter size size:** f X f <br>\n",
    "**Output size:** (n+2p-f+1) X (n+2p-f+1) <br>\n",
    "\n",
    "There are two common choices for padding:\n",
    "\n",
    "1. Valid: It means no padding. If we are using valid padding, the output will be (n-f+1) X (n-f+1)\n",
    "2. Same: Here, we apply padding so that the output size is the same as the input size, i.e., <br>\n",
    "n+2p-f+1 = n <br>\n",
    "So, p = (f-1)/2 <br>\n",
    "We now know how to use padded convolution. This way we don’t lose a lot of information and the image does not shrink either. \n",
    "\n",
    "### Strided Convolutions\n",
    "\n",
    "Suppose we choose a stride of 2. So, while convoluting through the image, we will take two steps – both in the horizontal and vertical directions separately. The dimensions for stride s will be:\n",
    "\n",
    "**Input size:** n X n <br>\n",
    "**Padding size:** p <br>\n",
    "**Stride:** s <br>\n",
    "**Filter size:** f X f <br>\n",
    "**Output size:** [(n+2p-f)/s+1] X [(n+2p-f)/s+1] <br>\n",
    "Stride helps to reduce the size of the image, a particularly useful feature.\n",
    "\n",
    "### Pooling Layers\n",
    "Pooling layers are generally used to reduce the size of the input image and to increase the speed of computation.\n",
    "\n",
    "Consider a 4 X 4 matrix as shown below:\n",
    "\n",
    "![alt](IMG6.png)\n",
    "\n",
    "Applying max pooling on this matrix will result in a 2 X 2 output:\n",
    "\n",
    "![alt](IMG7.png)\n",
    "\n",
    "For every 2 X 2 box, we take the maximum number. Here, we have applied a filter of size 2 and a stride of 2. These are the hyperparameters for the pooling layer. Apart from max pooling, we can also apply average pooling where, instead of taking the maximum of the numbers, we take their average.\n",
    "\n",
    "### Fully connected layer\n",
    "The result after applying a different filter is a matrix. We have to convert that matrix in the form of a vector to feed it into the neural network. For this, we make a fully connected layer. From the picture shown below, the 1st matrix is the result we get applying different filters and the second layer is the fully connected layer generated from the matrix.\n",
    "\n",
    "![alt](IMG8.png)\n",
    "\n",
    "After getting a fully connected layer, we pass this layer as an input layer to the neural network in order to get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Q1. Why FFNNs can not be used for images?</font>\n",
    "- FFNN does not take into account the `spatial relationship` between the pixels, so it fails to identify the patterns in the image pixels! as shown in below image \n",
    "![alt](convolution5.png)\n",
    "- If there is an object that we are interested in finding in the image, no matter at what position it is present in the image, we can use the same weights to identify that object. This property is known as `parameter sharing`, which also helps to capture the `translation invariance` property of images. We we will see it next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Introducing Convolutional Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class A - Rectange | Class B - Circle\n",
    "- | - \n",
    "![alt](convolution2.png) | ![alt](convolution3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='crimpson'>Ignore the dashed lines above in the rectangle, I didn't find any rectangle picture online and assume both of these two images are colored images. What according to you makes these images different from each other? Think from image feature extraction point of view</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='crimpson'>In the above rectange image, there are two `horizontal images`, based on your understanding - what do you think do we need to have two separate filters to identify these two horizontal edges or only one filter can alone identify these two horizontal edges?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution4.gif \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above convolution is known as 2D convolution as the image has only two dimensions. But for an `RGB` the covolution will be a 3D convolution. So what will be the shape of the output when we convolve the colored images of rectange/circle by using 3 different filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the formula to compute output of a convolution operation is -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n + 2p -f}{s} + 1$$\n",
    "we need to `floor` of the above value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where - \n",
    "- n is image size\n",
    "- p is padding\n",
    "- f is filter size\n",
    "- s is stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution6.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Q2. In the above convolution, does all the pixels contributed equally to compute the convolved feature?</font>\n",
    "### <font color='crimpson'>Q3. And what will happen if move the filter faster by taking a long step instead of taking a unit step? Which is also known as `stride`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go back to slide no #62 and #63 and understand this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what `padding` and `stride` is, please try to compute the output shape of convolved feature of the below convolution (assume we not using any padding and unit stride) -\n",
    "![alt text](convolution1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Solution</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolved feature for Class A - rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution7.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolved feature for Class B - circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution9.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging the above filters in proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution10.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What happens when we have complex objects in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution11.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution12.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Q5. If we have 16 filters of dimension 3x3x3, then how many parameters the CNN would have?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Schematic of a CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](convolution13.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Example features learnt by filters - </font>\n",
    "![alt text](convolution16.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Pooling layers</font>\n",
    "Pooling layer helps to progressively `reduce the spatial size of the convolved feature` to **reduce** the amount of `parameters` and `computation` in the network. There are two types of pooling - \n",
    "- Max Pooling\n",
    "- Average Pooling\n",
    "![alt text](convolution14.png \"Title\")\n",
    "![alt text](convolution15.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Fully Connected Layers</font>\n",
    "These are the standard densely connected layer that we have seen before in FFNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>Image pre-processing steps - </font>\n",
    "- Removing noise from images e.g. background extraction (we have seen already - Otsu's binarization)\n",
    "- Using normalization in images. Why?\n",
    "- Image augmentation (we will see in a different notebook)\n",
    "- Resizing CNN architecture's required input (we will see in a different notebook)\n",
    "- and many more... depending on the business problem at your hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>How to tune CNN?</font>\n",
    "- filter size\n",
    "- padding\n",
    "- stride\n",
    "- number of filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='crimpson'>References</font>\n",
    "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0263-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
